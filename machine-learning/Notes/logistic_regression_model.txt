Cost function
-------------

How do we come up with the paramter theta?

cost for linear regression
J = 1/m * (sum for all samples) [ 1/2 * (h (theta) (x) - y ) ^ 2]

cost = 1/2  * (h (theta) (x) - y) ^ 2

This function does not work for logistic regression since it is non-convex i.e.
there are multiple local minimas. We need to have a function that is convex
that has just one local minima which also happens to be the global minima of the
cost function.

Cost function for logistic regression

cost = -log(h(x) if y =1
		-log(1-h(x)) if y = 0

This cost function is desirable for the following properties:

When y = 1
1) if y = 1 and h(x) = 1, cost = 0 i.e. prediction is correct.
2) if h(x) -> 0 , cost tends to infinity.
3) if h(x) = 0, and y =1, (worst prediction), cost tends to infinity
meaning we will penalize the learning algo by a large cost.

When y = 0
1) when y =0, h(x) = 0, learning algo nailed it
2) when y = 0 and h(x) = 1 (worst prediction), cost tends to infinity



